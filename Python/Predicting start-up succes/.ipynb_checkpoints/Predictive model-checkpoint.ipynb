{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c3c127",
   "metadata": {},
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7216e",
   "metadata": {},
   "source": [
    "The following code will allow you to load and install all required packages, libraries and functions in order to run the code in the notebook. If a package is not installed on your computer, a line of code is provided as a comment that can be used to install the package with via the command prompts of Conda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd91ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tld import get_tld\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.color'] = 'black'\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "085689f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .csv files\n",
    "dforg = pd.read_csv('dforgbonus_final.csv', index_col = 0)\n",
    "dfinv = pd.read_csv('dfinv_final.csv', index_col = 0)\n",
    "dfacq = pd.read_csv('dfacq_final.csv', index_col = 0)\n",
    "dfrou = pd.read_csv('dfrou_final.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cb40d",
   "metadata": {},
   "source": [
    "## End of data prep and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d5158",
   "metadata": {},
   "source": [
    "# Predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b31d9",
   "metadata": {},
   "source": [
    "In this section we build a predictive model that can estimate the amount of funding start-ups receive. First we need to create the dataframe for the model by taking key variables from all of the other sheets. Thereafter we use 5-fold cross-validation and test data to create a model that generalises well and does not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4805c6c",
   "metadata": {},
   "source": [
    "## Creating the dataframe for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e510ad8",
   "metadata": {},
   "source": [
    "### Data from organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae37f5",
   "metadata": {},
   "source": [
    "The whole dforg dataframe will be used as basis for the model master sheet. This because it has the correct permalink to combine all other sheets. Later on, when everything is added, only the necessary columns will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "000a0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmodel = dforg.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31cd960",
   "metadata": {},
   "source": [
    "### Data from investment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a3f30",
   "metadata": {},
   "source": [
    "A new variable needs to be added, namely the percentage of investor type i.e. organization or person.  The percentage indicates the amount of funding that came from organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "898aaaac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First create a variable where organization is equal to 0\n",
    "dfinv['org_bin'] = np.where(dfinv['investor_type_x'] == 'organization', 1,0)\n",
    "# The organizations are grouped together and the percentages are calculated by taking the sum and dividing that by the count.\n",
    "dfinv['O/P_%'] = dfinv.groupby('company_permalink')['org_bin'].transform('sum') / dfinv.groupby('company_permalink')['org_bin'].transform('count')\n",
    "# Keep only the first sample to avoid duplicates\n",
    "dfinv=dfinv.groupby('company_permalink').first().reset_index()\n",
    "\n",
    "# Merge\n",
    "dfmodel=pd.merge(dfmodel,dfinv[['company_permalink','O/P_%']],how='left',left_on='permalink',right_on='company_permalink')\n",
    "dfmodel.drop('company_permalink',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a961e18",
   "metadata": {},
   "source": [
    "### Data from acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52dec4",
   "metadata": {},
   "source": [
    "Extra data is mainly centered around the average time to acquisition from founding and from last funding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34d88c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the average time from companies that were acquired more than one time\n",
    "dfacq['avg_time_acq_last_funding']=dfacq.groupby('company_permalink')['time_acq_last_funding'].transform('mean')\n",
    "dfacq['avg_time_founding_acq']=dfacq.groupby('company_permalink')['time_founding_acq'].transform('mean')\n",
    "dfacq=dfacq.groupby('company_permalink').first().reset_index()\n",
    "\n",
    "# Merge\n",
    "dfmodel = pd.merge(dfmodel, dfacq[['company_permalink','time_acq_last_funding','time_founding_acq']],how='left',left_on='permalink',right_on='company_permalink')\n",
    "dfmodel.drop('company_permalink',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779d620",
   "metadata": {},
   "source": [
    "### Data from round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685cfdaa",
   "metadata": {},
   "source": [
    "The only extra data we want to extract from here is the last type of funding that each company received i.e. series A, B, C, seed etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e25fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values to extract the final funding type of each company\n",
    "dfrou.sort_values(by='funded_at',ascending=False,inplace=True)\n",
    "dfrou=dfrou.groupby('company_permalink').first().reset_index()\n",
    "\n",
    "# Merge to get the final funding round types for each company.\n",
    "dfmodel = pd.merge(dfmodel,dfrou[['company_permalink','funding_round_type']],how='left',left_on='permalink',right_on='company_permalink')\n",
    "dfmodel.drop('company_permalink',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c7309",
   "metadata": {},
   "source": [
    "### Change markets in the bottom 20% to 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7639c",
   "metadata": {},
   "source": [
    "As there are a lot of different markets to be found, we decided to combine the largest chunk of these together. We decided on an 80/20 split where the top 80% are still grouped by their market and the remaining 20% is grouped together and named \"other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b16e4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe to get the cummulative percentage of the top markets\n",
    "dfmarketcount = dfmodel['market'].value_counts().rename_axis('market').reset_index(name = 'count')\n",
    "dfmarketcount['cum_percent'] = 100 * (dfmarketcount['count'].cumsum()/dfmarketcount['count'].sum())\n",
    "\n",
    "# At which index is the threshold of 80% reached\n",
    "dfmarketcount[dfmarketcount['cum_percent'].gt(80)].index[0]  # From this we see that the top 75 markets make up 80% of the entries.\n",
    "\n",
    "# Create list with top 74 markets\n",
    "marketlink = list(dfmarketcount['market'].head(75))\n",
    "\n",
    "# Change all other markets to 'other'\n",
    "dfmodel.loc[~dfmodel['market'].isin(marketlink), 'market'] = 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc10a45",
   "metadata": {},
   "source": [
    "### Change domains in the bottom 2% to 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2a8cc",
   "metadata": {},
   "source": [
    "As there are a lot of different domains to be found, we decided to combine the largest chunk of these together. We decided on an 98/2 split where the top 98% are still grouped by their domain and the remaining 2% is grouped together and named \"other\". This split was chosen as the top 98% are less than 30 different domains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1f5b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe to get the cummulative percentage of the top domains\n",
    "dfdomaincount = dforg['Domains'].value_counts().rename_axis('Domains').reset_index(name = 'count')\n",
    "dfdomaincount['cum_percent'] = 100 * (dfdomaincount['count'].cumsum()/dfmarketcount['count'].sum())\n",
    "\n",
    "# At which index is the threshold of 98% reached\n",
    "dfdomaincount[dfdomaincount['cum_percent'].gt(98)].index[0]  # From this we see that the top 29 domains make up 98% of the entries.\n",
    "\n",
    "# Create list with top 29 domains\n",
    "domainlink = list(dfdomaincount['Domains'].head(29))\n",
    "\n",
    "# Change all other domains to 'other'\n",
    "dforg.loc[~dforg['Domains'].isin(domainlink), 'Domains'] = 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c207c8",
   "metadata": {},
   "source": [
    "### Change all non-US states to 'Non-US'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e192fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all non-US states to 'Non-US', this because we believe the states only to be interesting for the US.\n",
    "dforg.loc[dforg['country_code'] != 'USA', 'state_code'] = 'Non-US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ec5d9",
   "metadata": {},
   "source": [
    "### Transform categorical variables and years to dummies to run the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307531c",
   "metadata": {},
   "source": [
    "One hot encoding can be defined as the process of changing categorical data variables to be fed into machine and deep learning algorithms, which improves the model's prediction and classification accuracy. Preprocessing categorical features for machine learning models using one hot encoding is a frequent practice.\n",
    "This form of encoding creates a new binary feature for each potential category and assigns a value of 1 to each sample's feature that matches to its original category.\n",
    "\n",
    "We chose to encode the following variables to dummy variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34f59a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funding round type\n",
    "round_type = pd.get_dummies(dfmodel['funding_round_type'])\n",
    "dfmodel = pd.concat([dfmodel,round_type],axis=1)\n",
    "dfmodel.drop('funding_round_type',axis=1,inplace=True)\n",
    "\n",
    "# Country code\n",
    "country_code = pd.get_dummies(dfmodel['country_code'])\n",
    "dfmodel = pd.concat([dfmodel,country_code],axis=1)\n",
    "dfmodel.drop('country_code',axis=1,inplace=True)\n",
    "\n",
    "# Market\n",
    "market = pd.get_dummies(dfmodel['market'])\n",
    "dfmodel = pd.concat([dfmodel,market],axis=1)\n",
    "dfmodel.drop('market',axis=1,inplace=True)\n",
    "\n",
    "# State code\n",
    "state_code = pd.get_dummies(dfmodel['state_code'])\n",
    "dfmodel = pd.concat([dfmodel,state_code],axis=1)\n",
    "dfmodel.drop('state_code',axis=1,inplace=True)\n",
    "\n",
    "# Domains\n",
    "domains = pd.get_dummies(dfmodel['Domains'])\n",
    "dfmodel = pd.concat([dfmodel,domains],axis=1)\n",
    "dfmodel.drop('Domains',axis=1,inplace=True)\n",
    "\n",
    "# First funding year\n",
    "first_funding = pd.get_dummies(dfmodel['first_funding_year'])\n",
    "dfmodel = pd.concat([dfmodel,first_funding],axis=1)\n",
    "dfmodel.drop('first_funding_year',axis=1,inplace=True)\n",
    "\n",
    "# Last funding year\n",
    "last_funding = pd.get_dummies(dfmodel['last_funding_year'])\n",
    "dfmodel = pd.concat([dfmodel,last_funding],axis=1)\n",
    "dfmodel.drop('last_funding_year',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a423b9",
   "metadata": {},
   "source": [
    "## Normalize variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ab9ab",
   "metadata": {},
   "source": [
    "As there are some variables (i.e. year and the differing amounts) where the range between entries is too large, a form of normalization needs to take place. Many machine learning algorithms perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed.\n",
    "\n",
    "We decided to use the MinMaxScaler. MinMaxScaler subtracts the feature's minimum value from each value in the feature, then divides by the range. The range is the difference between the maximum and minimum values.\n",
    "\n",
    "The shape of the original distribution is preserved by MinMaxScaler. It has no effect on the information included in the original data.\n",
    "\n",
    "It's worth noting that MinMaxScaler doesn't lessen the significance of outliers.\n",
    "\n",
    "MinMaxScaler returns a feature with a default range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e686f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "\n",
    "dfmodel[['founding_year','DaysBetweenFunding','DaysBetweenFoundFund','DaysPerRound','gdpFOU','gdpFUNF',\n",
    "        'gdpFUNL','gdp95','gdp15','gdp95_15','gdpFOU_FUNL','gdpFUNF_FUNL','gdppcFOU','gdppcFUNF',\n",
    "        'gdppc95','gdppc15','gdppc95_15','gdppcFOU_FUNL','gdppcFUNF_FUNL','r&dFOU','r&dFUNF','r&dFUNL',\n",
    "        'r&d95','r&d15','r&d95_15','r&dFOU_FUNL','r&dFUNF_FUNL','time_acq_last_funding',\n",
    "        'time_founding_acq']] = mms.fit_transform(dfmodel[['founding_year','DaysBetweenFunding','DaysBetweenFoundFund','DaysPerRound','gdpFOU','gdpFUNF',\n",
    "        'gdpFUNL','gdp95','gdp15','gdp95_15','gdpFOU_FUNL','gdpFUNF_FUNL','gdppcFOU','gdppcFUNF',\n",
    "        'gdppc95','gdppc15','gdppc95_15','gdppcFOU_FUNL','gdppcFUNF_FUNL','r&dFOU','r&dFUNF','r&dFUNL',\n",
    "        'r&d95','r&d15','r&d95_15','r&dFOU_FUNL','r&dFUNF_FUNL','time_acq_last_funding','time_founding_acq']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9b274",
   "metadata": {},
   "source": [
    "## Split dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6599ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "dfmodel.drop(['permalink','name','homepage_url','category_list','founded_at','first_funding_at',\n",
    "              'last_funding_at','region','city', 'gdp95', 'gdp15', 'gdp95_15', 'gdppc95', 'gdppc15', \n",
    "              'gdppc95_15', 'r&d95', 'r&d15', 'r&d95_15'],axis=1,inplace=True)\n",
    "\n",
    "#Replace NaN for 0 to avoid ValueError: Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "dfmodel = dfmodel.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "\n",
    "#Split set into features and target variable\n",
    "x = dfmodel.drop('funding_total_usd',axis=1)\n",
    "y = dfmodel[['funding_total_usd']]\n",
    "\n",
    "#We also define random_state which corresponds to the seed, so that results are reproducible\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd357cb",
   "metadata": {},
   "source": [
    "## Running OLS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f60e50",
   "metadata": {},
   "source": [
    "The objective of a linear regression model is to find a relationship between one or more features(independent variables) and a continuous target variable(dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fca4d2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the initial model to establish a baseline\n",
    "# Create a based model\n",
    "modelOLS = LinearRegression(fit_intercept = True)\n",
    "\n",
    "modelOLS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "670fb43b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-57941031.91803935\n",
      "23871830230.74381\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_ols = modelOLS.predict(x_test)\n",
    "# Print performance metrics\n",
    "print(r2_score(y_test, y_pred_ols))\n",
    "print(mean_absolute_error(y_test,y_pred_ols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5550587",
   "metadata": {},
   "source": [
    "The values of the performance metrics shown above are quite large and unhelpful, especially the R-squared. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99897a17",
   "metadata": {},
   "source": [
    "## Running Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0870fe7",
   "metadata": {},
   "source": [
    "The main purpose in Lasso Regression is to find the coefficients that minimize the error sum of squares by applying a penalty to these coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81ac49f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6669710343800275e+19, tolerance: 6828222997296592.0\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "modelLasso = Lasso(max_iter=10000).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd9c28cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13161494951420794\n",
      "16978982.44416478\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_lasso = modelLasso.predict(x_test)\n",
    "# Print performance metrics\n",
    "print(r2_score(y_test, y_pred_lasso))\n",
    "print(mean_absolute_error(y_test, y_pred_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fae81",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d586f",
   "metadata": {},
   "source": [
    "To see whether this score can be improved we choose to do a grid search for hyperparameter tuning. Hyperparameter is a parameter whose value is used to contrl the learning process, and hyper-parameter tuning means choosing the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "159e781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4260222547610049e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7808170510815592e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7300053882894862e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7573508165597004e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6393899229463022e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.714258826486514e+18, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2125891510731063e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2058877746773934e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.169150107627807e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1578173299212632e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.236221519027403e+19, tolerance: 5651954704740940.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2401648361328542e+19, tolerance: 5074241676079120.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5947096197439328e+19, tolerance: 5074241676079120.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7194622100519193e+19, tolerance: 5074241676079120.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8502964569976107e+19, tolerance: 5074241676079120.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8548352655690183e+19, tolerance: 5074241676079120.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.823422619694991e+19, tolerance: 5074241676079120.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8761672213326557e+19, tolerance: 5074241676079120.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.009365437827727e+18, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4820510372232847e+18, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0387936446371103e+18, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8556483736001126e+18, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4828334578324316e+18, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.999503737012142e+17, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.726526989963981e+18, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5491115421008255e+19, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.893909711005286e+16, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4034346486996992e+16, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.030589279627756e+17, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8627777249771016e+19, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.732458469019812e+18, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4818264303379522e+19, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.938159154900297e+19, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.960435391641526e+19, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.963064636823058e+19, tolerance: 4992217958029685.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8967719068753920.0, tolerance: 5722333657818207.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2393073492732838e+19, tolerance: 5722333657818207.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.191118773486074e+19, tolerance: 5722333657818207.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2185147476430057e+19, tolerance: 5722333657818207.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.236431756631188e+19, tolerance: 5722333657818207.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2259959720512635e+19, tolerance: 5722333657818207.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2363809074193293e+19, tolerance: 5722333657818207.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2430783745603523e+19, tolerance: 5722333657818207.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.1573215304071e+18, tolerance: 5872065190943090.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.004110334422057e+17, tolerance: 5872065190943090.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.167948801304166e+16, tolerance: 5872065190943090.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.268506482613617e+17, tolerance: 5872065190943090.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2780140484661223e+19, tolerance: 5872065190943090.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3001859111740047e+19, tolerance: 5872065190943090.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using 5 fold cross-validation.\n",
    "lasso_cv_model = LassoCV(alphas = np.random.randint(0,1000,100), cv = 5, max_iter = 10000).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b10e3387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "987"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View best alpha value\n",
    "lasso_cv_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e9660f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the corrected Lasso model with optimum alpha value\n",
    "modelLasso_tuned = Lasso(max_iter = 10000).set_params(alpha = lasso_cv_model.alpha_).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8fe9f5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13050090355335842\n",
      "16865860.672136225\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using tuned model\n",
    "y_pred_lasso_tuned = modelLasso_tuned.predict(x_test)\n",
    "# Print performance metrics\n",
    "print(r2_score(y_test, y_pred_lasso_tuned))\n",
    "print(mean_absolute_error(y_test, y_pred_lasso_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1939d21",
   "metadata": {},
   "source": [
    "As we see here, the model performs worse after tuning than before so the R-squared of the Random Forest will be taken as __0.1316__, which means that the independent variables in the Lasso Regression Model explain 13.16% of the change in the dependent variable for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc6474",
   "metadata": {},
   "source": [
    "## Running Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81aea62",
   "metadata": {},
   "source": [
    "Random forest is a supervised machine learning algorithm that is commonly used to solve classification and regression problems. It creates decision trees from various samples, using the majority vote for classification and the average for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "172707a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-2b4a8d42af39>:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(x_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the initial model to establish a baseline\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e43c1ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15628480021642832\n",
      "13824531.008093499\n"
     ]
    }
   ],
   "source": [
    "print(rf.score(x_test,y_test))\n",
    "print(mean_absolute_error(y_test,rf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b83cfc",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebb1a6",
   "metadata": {},
   "source": [
    "To see whether this score can be improved we choose to do a grid search for hyperparameter tuning. Hyperparameter is a parameter whose value is used to contrl the learning process, and hyper-parameter tuning means choosing the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0371dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:880: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [2, 4, 6, None],\n",
       "                         'max_features': ['auto', 'sqrt'],\n",
       "                         'n_estimators': [100, 200, 300, 400, 500]},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto','sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 6, num = 3 )]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "#min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "#min_samples_leaf = [1, 2]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True] #False?\n",
    "\n",
    "# Create the parameter grid to tune the hyper-parameters \n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               #'min_samples_split': min_samples_split,\n",
    "               #'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 10)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab93ff4",
   "metadata": {},
   "source": [
    "### View the best parameters from fitting the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14f93438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'n_estimators': 400}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b91b8",
   "metadata": {},
   "source": [
    "### Evaluate the Best Random Forest Model from Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b68b93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11850848198958486\n",
      "14066650.499949815\n"
     ]
    }
   ],
   "source": [
    "#Select best model\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "#Test model performance\n",
    "print(best_grid.score(x_test,y_test))\n",
    "print(mean_absolute_error(y_test,best_grid.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d80868",
   "metadata": {},
   "source": [
    "As we see here, the model performs worse after tuning than before so the R-squared of the Random Forest will be taken as __0.1562__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5463f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funding_rounds</td>\n",
       "      <td>5.203122e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DaysBetweenFoundFund</td>\n",
       "      <td>4.136170e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DaysBetweenFunding</td>\n",
       "      <td>3.988007e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DaysPerRound</td>\n",
       "      <td>3.952236e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>private_equity</td>\n",
       "      <td>3.675577e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>BLR</td>\n",
       "      <td>1.541648e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>CRI</td>\n",
       "      <td>6.953475e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>TUN</td>\n",
       "      <td>5.763249e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>tk</td>\n",
       "      <td>1.572190e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>lk</td>\n",
       "      <td>3.044921e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>482 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Attribute    Importance\n",
       "0          funding_rounds  5.203122e-02\n",
       "12   DaysBetweenFoundFund  4.136170e-02\n",
       "11     DaysBetweenFunding  3.988007e-02\n",
       "13           DaysPerRound  3.952236e-02\n",
       "53         private_equity  3.675577e-02\n",
       "..                    ...           ...\n",
       "70                    BLR  1.541648e-10\n",
       "80                    CRI  6.953475e-11\n",
       "152                   TUN  5.763249e-11\n",
       "454                    tk  1.572190e-11\n",
       "402                    lk  3.044921e-12\n",
       "\n",
       "[482 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get importance\n",
    "importances = pd.DataFrame(data={\n",
    "    'Attribute': x_train.columns,\n",
    "    'Importance': best_grid.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "print(importances.shape[0])\n",
    "importances[importances['Importance'] >0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182bcce7",
   "metadata": {},
   "source": [
    "## Running Support Vector Machine Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a45e19f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gijst\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(kernel='linear')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "modelSVM = SVR(kernel = 'linear')  # We chose for the linear kernel as we have many features in our data set.\n",
    "\n",
    "modelSVM.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96e4e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02407807177428234\n",
      "15557847.645472461\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_SVM = modelSVM.predict(x_test)\n",
    "# Print performance metrics\n",
    "print(r2_score(y_test, y_pred_SVM))\n",
    "print(mean_absolute_error(y_test, y_pred_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ac89d",
   "metadata": {},
   "source": [
    "Due to the bad performance metric scores we did not decide to pursue further tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3d2c0",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d165873",
   "metadata": {},
   "source": [
    "Next, we try the gradient boosting method to estimate the amount of funding startups receive. This method is builds the model additively and fits a regression tree on the negative gradient of a loss function in each stage. \n",
    "It can be categorized as an ensemble method, where predictions of multiple base estimators are combined to improve the generalizability. As our goal is to create a model that generalizes well and does not overfit, this is an appropriate method to consider.\n",
    "\n",
    "In contrast to the random forests where predictions of single estimators are averaged after they have been built independently, in gradient boosting, these estimators are built sequentially and try to reduce the bias of the combined estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f887f4",
   "metadata": {},
   "source": [
    "## Gradient Boosting with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534bcb6",
   "metadata": {},
   "source": [
    "With secondary data included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8fd2efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14015555652364742\n",
      "14215340.714933703\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "params = {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'criterion': 'mse'}\n",
    "gradient_boosting_regressor_model = GradientBoostingRegressor(**params)\n",
    "y_train = np.asarray(y_train['funding_total_usd'])\n",
    "gradient_boosting_regressor_model.fit(x_train, y_train)\n",
    "y_pred = gradient_boosting_regressor_model.predict(x_test)\n",
    "y_pred\n",
    "\n",
    "#Test score\n",
    "gradient_boosting_regressor_model.score(x_test, y_test)\n",
    "\n",
    "#R^2\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e2545",
   "metadata": {},
   "source": [
    "## Gradient Boosting using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f9b6f",
   "metadata": {},
   "source": [
    "XGBoost is a more regularized form of Gradient Boosting and is known for picking up patterns and regularities in the data. It uses advanced regularization (L1 & L2), which improves model generalization capabilities. Therefore, it tends to deliver higher performance compared to gradient boosting. \n",
    "This is achieved by improvisations made on the gradient boosting framework, by introducing more accurate approximations that avoid overfitting and help find the best tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d047dd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.2-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mgarm\\anaconda3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\mgarm\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "641144c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find duplicate columns\n",
    "#duplicate_columns = x_train.columns[x_train.columns.duplicated()]\n",
    "#duplicate_columns\n",
    "#Change names/drop duplicate columns\n",
    "x_train.columns.values[512] = \"lf_2014\"\n",
    "x_train.columns.values[511] = \"lf_2013\"\n",
    "x_train.columns.values[510] = \"lf_2012\"\n",
    "x_train.columns.values[509] = \"lf_2011\"\n",
    "x_train.columns.values[508] = \"lf_2010\"\n",
    "x_train.columns.values[507] = \"lf_2009\"\n",
    "x_train.columns.values[506] = \"lf_2008\"\n",
    "x_train.columns.values[505] = \"lf_2007\"\n",
    "x_train.columns.values[504] = \"lf_2006\"\n",
    "x_train.columns.values[503] = \"lf_2005\"\n",
    "x_train.columns.values[502] = \"lf_2004\"\n",
    "x_train.columns.values[501] = \"lf_2003\"\n",
    "x_train.columns.values[500] = \"lf_2002\"\n",
    "x_train.columns.values[499] = \"lf_2001\"\n",
    "x_train.columns.values[498] = \"lf_2000\"\n",
    "x_train.columns.values[497] = \"lf_1999\"\n",
    "x_train.columns.values[496] = \"lf_1998\"\n",
    "x_train.columns.values[495] = \"lf_1997\"\n",
    "x_train.columns.values[494] = \"lf_1996\"\n",
    "x_train.columns.values[493] = \"lf_1994\"\n",
    "x_train.columns.values[492] = \"lf_1990\"\n",
    "x_train.drop(x_train.columns[15], axis=1,inplace=True)\n",
    "\n",
    "x_test.columns.values[512] = \"lf_2014\"\n",
    "x_test.columns.values[511] = \"lf_2013\"\n",
    "x_test.columns.values[510] = \"lf_2012\"\n",
    "x_test.columns.values[509] = \"lf_2011\"\n",
    "x_test.columns.values[508] = \"lf_2010\"\n",
    "x_test.columns.values[507] = \"lf_2009\"\n",
    "x_test.columns.values[506] = \"lf_2008\"\n",
    "x_test.columns.values[505] = \"lf_2007\"\n",
    "x_test.columns.values[504] = \"lf_2006\"\n",
    "x_test.columns.values[503] = \"lf_2005\"\n",
    "x_test.columns.values[502] = \"lf_2004\"\n",
    "x_test.columns.values[501] = \"lf_2003\"\n",
    "x_test.columns.values[500] = \"lf_2002\"\n",
    "x_test.columns.values[499] = \"lf_2001\"\n",
    "x_test.columns.values[498] = \"lf_2000\"\n",
    "x_test.columns.values[497] = \"lf_1999\"\n",
    "x_test.columns.values[496] = \"lf_1998\"\n",
    "x_test.columns.values[495] = \"lf_1997\"\n",
    "x_test.columns.values[494] = \"lf_1996\"\n",
    "x_test.columns.values[493] = \"lf_1994\"\n",
    "x_test.columns.values[492] = \"lf_1990\"\n",
    "x_test.drop(x_test.columns[15], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efd481",
   "metadata": {},
   "source": [
    "With secondary data included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56879a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1865698193348978\n",
      "14606070.948245412\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "#drop features with the same name\n",
    "\n",
    "params = {'max_depth': 8, 'subsample': 0.8, 'lambda': 3}\n",
    "XGB_model = XGBRegressor(**params)\n",
    "XGB_model.fit(x_train, y_train)\n",
    "y_pred = XGB_model.predict(x_test)\n",
    "#Test score\n",
    "XGB_model.score(x_test, y_test)\n",
    "\n",
    "#R^2\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4cb45a",
   "metadata": {},
   "source": [
    "We thus see that the model increased in performance with the XGBoost algorithm  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b7483",
   "metadata": {},
   "source": [
    "# K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb86455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017088942302536436\n",
      "17948528.16029238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "np.random.seed(1234)\n",
    "params = {'n_neighbors': 15}\n",
    "knn_model = KNeighborsRegressor(**params)\n",
    "#knn_model = KNeighborsRegressor()\n",
    "#y_trainGB = np.asarray(y_trainGB['funding_total_usd'])\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_pred = knn_model.predict(x_test)\n",
    "y_pred\n",
    "\n",
    "#Test score\n",
    "knn_model.score(x_test, y_test)\n",
    "\n",
    "#R^2\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38bcff2",
   "metadata": {},
   "source": [
    "Due to its inferior performance compared to the ensemble methods, this method does not seem worth investigating further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d1080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
